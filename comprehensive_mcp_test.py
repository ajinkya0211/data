#!/usr/bin/env python3
"""
Comprehensive MCP AI Agent System Test
Tests the entire system with detailed logging of all aspects
"""

import asyncio
import json
import requests
import time
import os
import sys
from datetime import datetime
from typing import Dict, Any, List
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
BASE_URL = "http://localhost:8000"
TEST_RESULTS_DIR = "test_results"
AUTH_TOKEN = None

# Test session tracking
test_session = {
    "start_time": datetime.now().isoformat(),
    "tests_run": [],
    "errors": [],
    "warnings": [],
    "success_count": 0,
    "failure_count": 0
}

def log_to_file(filename: str, content: str, mode: str = "w"):
    """Log content to a file in the test results directory"""
    filepath = os.path.join(TEST_RESULTS_DIR, filename)
    with open(filepath, mode) as f:
        f.write(content)
    print(f"ğŸ“ Logged to: {filepath}")

def log_test_result(test_name: str, success: bool, details: Dict[str, Any]):
    """Log test results to the session"""
    test_result = {
        "name": test_name,
        "success": success,
        "timestamp": datetime.now().isoformat(),
        "details": details
    }
    test_session["tests_run"].append(test_result)
    
    if success:
        test_session["success_count"] += 1
    else:
        test_session["failure_count"] += 1

def get_auth_headers():
    """Get authentication headers"""
    if AUTH_TOKEN:
        return {"Authorization": f"Bearer {AUTH_TOKEN}"}
    return {}

def log_api_response(endpoint: str, response: requests.Response, test_name: str):
    """Log API response details"""
    response_data = {
        "endpoint": endpoint,
        "status_code": response.status_code,
        "headers": dict(response.headers),
        "timestamp": datetime.now().isoformat(),
        "test_name": test_name
    }
    
    try:
        response_data["body"] = response.json() if response.content else None
    except:
        response_data["body"] = response.text if response.content else None
    
    # Log to API responses folder
    filename = f"api_responses/{test_name}_{endpoint.replace('/', '_')}_{datetime.now().strftime('%H%M%S')}.json"
    log_to_file(filename, json.dumps(response_data, indent=2))
    
    return response_data

async def test_system_health():
    """Test basic system health and connectivity"""
    print("\nğŸ¥ Testing System Health...")
    
    try:
        # Test basic connectivity
        response = requests.get(f"{BASE_URL}/docs", timeout=10)
        if response.status_code == 200:
            print("   âœ… Backend is accessible")
            log_test_result("system_health", True, {"status": "Backend accessible"})
        else:
            print(f"   âŒ Backend returned status: {response.status_code}")
            log_test_result("system_health", False, {"status_code": response.status_code})
            return False
            
        # Test API endpoints availability
        endpoints_to_test = [
            "/api/v1/mcp-ai/test",
            "/api/v1/mcp-ai/tools",
            "/api/v1/mcp-ai/execution-history"
        ]
        
        for endpoint in endpoints_to_test:
            try:
                response = requests.get(f"{BASE_URL}{endpoint}", timeout=10)
                if response.status_code in [200, 401, 403]:  # 401/403 means endpoint exists but needs auth
                    print(f"   âœ… Endpoint {endpoint} is accessible")
                else:
                    print(f"   âŒ Endpoint {endpoint} returned: {response.status_code}")
            except Exception as e:
                print(f"   âŒ Endpoint {endpoint} failed: {str(e)}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ System health test failed: {str(e)}")
        log_test_result("system_health", False, {"error": str(e)})
        return False

async def test_mcp_ai_agent_status():
    """Test MCP AI Agent system status"""
    print("\nğŸ¤– Testing MCP AI Agent Status...")
    
    try:
        response = requests.post(
            f"{BASE_URL}/api/v1/mcp-ai/test",
            headers=get_auth_headers(),
            timeout=30
        )
        
        # Log the response
        response_data = log_api_response("/api/v1/mcp-ai/test", response, "mcp_ai_status")
        
        if response.status_code == 200:
            data = response.json()
            print(f"   âœ… System Status: {data['data']['system_status']}")
            print(f"   âœ… Available Tools: {data['data']['available_tools']}")
            print(f"   âœ… Tool Categories: {', '.join(data['data']['tool_categories'])}")
            print(f"   âœ… AI Provider: {'âœ…' if data['data']['ai_provider'] else 'âŒ'}")
            
            # Log detailed status
            log_to_file("ai_logs/system_status.json", json.dumps(data, indent=2))
            
            log_test_result("mcp_ai_status", True, {
                "system_status": data['data']['system_status'],
                "available_tools": data['data']['available_tools'],
                "ai_provider": data['data']['ai_provider']
            })
            
            return True
        else:
            print(f"   âŒ Status check failed: {response.status_code}")
            print(f"   Response: {response.text}")
            
            log_test_result("mcp_ai_status", False, {
                "status_code": response.status_code,
                "response": response.text
            })
            
            return False
            
    except Exception as e:
        print(f"   âŒ MCP AI status test failed: {str(e)}")
        log_test_result("mcp_ai_status", False, {"error": str(e)})
        return False

async def test_tool_discovery():
    """Test AI tool discovery capabilities"""
    print("\nğŸ” Testing AI Tool Discovery...")
    
    try:
        response = requests.get(
            f"{BASE_URL}/api/v1/mcp-ai/tools",
            headers=get_auth_headers(),
            timeout=30
        )
        
        # Log the response
        response_data = log_api_response("/api/v1/mcp-ai/tools", response, "tool_discovery")
        
        if response.status_code == 200:
            data = response.json()
            tools = data['data']['tools']
            
            print(f"   âœ… Total Tools Available: {len(tools)}")
            print(f"   âœ… Tool Categories: {', '.join(data['data']['categories'])}")
            
            # Log detailed tool information
            log_to_file("actions/available_tools.json", json.dumps(data, indent=2))
            
            # Show some tool examples
            for i, tool in enumerate(tools[:3]):
                print(f"   ğŸ“‹ Tool {i+1}: {tool['name']}")
                print(f"      Category: {tool['category']}")
                print(f"      Description: {tool['description'][:60]}...")
                print(f"      Parameters: {len(tool['parameters'])} required")
            
            log_test_result("tool_discovery", True, {
                "total_tools": len(tools),
                "categories": data['data']['categories'],
                "tools_analyzed": len(tools[:3])
            })
            
            return True
        else:
            print(f"   âŒ Tool discovery failed: {response.status_code}")
            print(f"   Response: {response.text}")
            
            log_test_result("tool_discovery", False, {
                "status_code": response.status_code,
                "response": response.text
            })
            
            return False
            
    except Exception as e:
        print(f"   âŒ Tool discovery test failed: {str(e)}")
        log_test_result("tool_discovery", False, {"error": str(e)})
        return False

async def test_execution_history():
    """Test execution history retrieval"""
    print("\nğŸ“Š Testing Execution History...")
    
    try:
        response = requests.get(
            f"{BASE_URL}/api/v1/mcp-ai/execution-history",
            headers=get_auth_headers(),
            timeout=30
        )
        
        # Log the response
        response_data = log_api_response("/api/v1/mcp-ai/execution-history", response, "execution_history")
        
        if response.status_code == 200:
            data = response.json()
            history = data['data']
            
            print(f"   âœ… Total Executions: {history['total_executions']}")
            print(f"   âœ… Recent Executions: {len(history['recent_executions'])}")
            
            # Log detailed execution history
            log_to_file("actions/execution_history.json", json.dumps(data, indent=2))
            
            # Show recent execution details
            for i, execution in enumerate(history['recent_executions'][:3]):
                print(f"   ğŸ“Š Execution {i+1}: {execution.get('tool_name', 'Unknown')}")
                print(f"      Success: {'âœ…' if execution.get('success') else 'âŒ'}")
                print(f"      Parameters: {len(execution.get('parameters', {}))} params")
            
            log_test_result("execution_history", True, {
                "total_executions": history['total_executions'],
                "recent_executions": len(history['recent_executions'])
            })
            
            return True
        else:
            print(f"   âŒ Execution history test failed: {response.status_code}")
            print(f"   Response: {response.text}")
            
            log_test_result("execution_history", False, {
                "status_code": response.status_code,
                "response": response.text
            })
            
            return False
            
    except Exception as e:
        print(f"   âŒ Execution history test failed: {str(e)}")
        log_test_result("execution_history", False, {"error": str(e)})
        return False

async def test_natural_language_processing():
    """Test natural language request processing"""
    print("\nğŸ—£ï¸ Testing Natural Language Processing...")
    
    test_requests = [
        {
            "name": "data_cleaning_workflow",
            "request": "Create a data cleaning workflow for my dataset and add visualization blocks",
            "project_id": "test_project_123"
        },
        {
            "name": "simple_analysis",
            "request": "Create a simple data analysis block",
            "project_id": "test_project_123"
        },
        {
            "name": "visualization_request",
            "request": "Generate a bar chart visualization for my data",
            "project_id": "test_project_123"
        }
    ]
    
    results = []
    
    for test_req in test_requests:
        print(f"\n   ğŸ”„ Testing: {test_req['name']}")
        print(f"      Request: {test_req['request']}")
        
        try:
            response = requests.post(
                f"{BASE_URL}/api/v1/mcp-ai/process",
                headers={**get_auth_headers(), "Content-Type": "application/json"},
                json={
                    "request": test_req["request"],
                    "project_id": test_req["project_id"]
                },
                timeout=60
            )
            
            # Log the response
            response_data = log_api_response(
                "/api/v1/mcp-ai/process", 
                response, 
                f"nlp_{test_req['name']}"
            )
            
            if response.status_code == 200:
                data = response.json()
                result = data['data']
                
                print(f"      âœ… Request Processed Successfully!")
                print(f"      âœ… AI Plan Generated: {len(result.get('ai_plan', {}).get('steps', []))} steps")
                print(f"      âœ… Tools Used: {len(result.get('execution_results', {}).get('steps_results', []))}")
                print(f"      âœ… Context Used: {result['context_used']['tools_available']} tools available")
                
                # Log detailed AI response
                log_to_file(f"ai_logs/nlp_response_{test_req['name']}.json", json.dumps(data, indent=2))
                
                # Log AI plan details
                ai_plan = result.get('ai_plan', {})
                if ai_plan:
                    log_to_file(f"actions/ai_plan_{test_req['name']}.json", json.dumps(ai_plan, indent=2))
                    print(f"      ğŸ§  AI Understanding: {ai_plan.get('understanding', 'N/A')[:80]}...")
                    print(f"      ğŸ“‹ Plan Summary: {ai_plan.get('plan_summary', 'N/A')[:80]}...")
                    print(f"      ğŸ¯ Estimated Complexity: {ai_plan.get('estimated_complexity', 'N/A')}")
                
                # Log execution results
                execution_results = result.get('execution_results', {})
                if execution_results:
                    log_to_file(f"actions/execution_results_{test_req['name']}.json", json.dumps(execution_results, indent=2))
                    print(f"      âš¡ Execution Summary: {execution_results.get('execution_summary', 'N/A')}")
                    print(f"      âœ… Overall Success: {execution_results.get('overall_success', False)}")
                
                # Log AI response
                ai_response = result.get('ai_response', {})
                if ai_response:
                    log_to_file(f"outputs/ai_response_{test_req['name']}.json", json.dumps(ai_response, indent=2))
                    print(f"      ğŸ¤– AI Response: {ai_response.get('summary', 'N/A')[:80]}...")
                    print(f"      ğŸ’¡ User Message: {ai_response.get('user_message', 'N/A')[:80]}...")
                
                results.append({
                    "test_name": test_req['name'],
                    "success": True,
                    "ai_plan_steps": len(result.get('ai_plan', {}).get('steps', [])),
                    "tools_used": len(result.get('execution_results', {}).get('steps_results', [])),
                    "overall_success": execution_results.get('overall_success', False) if execution_results else False
                })
                
            else:
                print(f"      âŒ Processing failed: {response.status_code}")
                print(f"      Response: {response.text}")
                
                results.append({
                    "test_name": test_req['name'],
                    "success": False,
                    "status_code": response.status_code,
                    "error": response.text
                })
                
        except Exception as e:
            print(f"      âŒ Test failed: {str(e)}")
            results.append({
                "test_name": test_req['name'],
                "success": False,
                "error": str(e)
            })
    
    # Log overall NLP test results
    log_to_file("ai_logs/nlp_test_results.json", json.dumps(results, indent=2))
    
    success_count = sum(1 for r in results if r['success'])
    log_test_result("natural_language_processing", success_count > 0, {
        "total_tests": len(results),
        "successful_tests": success_count,
        "results": results
    })
    
    return success_count > 0

async def test_context_generation():
    """Test MCP context generation for a project"""
    print("\nğŸ§  Testing MCP Context Generation...")
    
    try:
        # Test with a sample project ID
        project_id = "test_project_123"
        
        response = requests.get(
            f"{BASE_URL}/api/v1/mcp-ai/capabilities/{project_id}",
            headers=get_auth_headers(),
            timeout=30
        )
        
        # Log the response
        response_data = log_api_response(f"/api/v1/mcp-ai/capabilities/{project_id}", response, "context_generation")
        
        if response.status_code == 200:
            data = response.json()
            capabilities = data['data']
            
            print(f"   âœ… Agent Type: {capabilities['agent_type']}")
            print(f"   âœ… Context Awareness: {'âœ…' if capabilities['capabilities']['context_awareness'] else 'âŒ'}")
            print(f"   âœ… LLM Planning: {'âœ…' if capabilities['capabilities']['llm_powered_planning'] else 'âŒ'}")
            print(f"   âœ… Tool Execution: {'âœ…' if capabilities['capabilities']['tool_based_execution'] else 'âŒ'}")
            print(f"   âœ… Total Tools: {capabilities['total_tools']}")
            
            # Log detailed capabilities
            log_to_file("ai_logs/agent_capabilities.json", json.dumps(data, indent=2))
            
            if 'project_context' in capabilities:
                print(f"   âœ… Project Context Size: {capabilities['project_context']['context_size']} chars")
                print(f"   âœ… Blocks Analyzed: {capabilities['project_context']['blocks_analyzed']}")
                print(f"   âœ… Data Insights: {capabilities['project_context']['data_insights']}")
            
            log_test_result("context_generation", True, {
                "agent_type": capabilities['agent_type'],
                "capabilities": capabilities['capabilities'],
                "total_tools": capabilities['total_tools']
            })
            
            return True
            
        else:
            print(f"   âŒ Context generation failed: {response.status_code}")
            print(f"   Note: This might fail if project doesn't exist - that's expected for demo")
            
            log_test_result("context_generation", False, {
                "status_code": response.status_code,
                "note": "Expected failure for non-existent project"
            })
            
            return False
            
    except Exception as e:
        print(f"   âŒ Context generation test failed: {str(e)}")
        log_test_result("context_generation", False, {"error": str(e)})
        return False

async def test_tool_execution_simulation():
    """Test tool execution simulation"""
    print("\nâš™ï¸ Testing Tool Execution Simulation...")
    
    try:
        # Simulate tool execution by testing the tool engine directly
        # This would normally be done through the API, but we'll simulate it
        
        print("   ğŸ”„ Simulating tool execution scenarios...")
        
        # Test different tool categories
        tool_categories = [
            "data_manipulation",
            "block_management", 
            "workflow_control",
            "analysis",
            "visualization"
        ]
        
        simulation_results = []
        
        for category in tool_categories:
            print(f"      ğŸ“‹ Testing {category} tools...")
            
            # Simulate tool execution for each category
            simulation_result = {
                "category": category,
                "tools_available": True,
                "execution_simulated": True,
                "timestamp": datetime.now().isoformat()
            }
            
            simulation_results.append(simulation_result)
            print(f"         âœ… {category} tools simulation completed")
        
        # Log simulation results
        log_to_file("actions/tool_execution_simulation.json", json.dumps(simulation_results, indent=2))
        
        print(f"   âœ… Tool execution simulation completed for {len(tool_categories)} categories")
        
        log_test_result("tool_execution_simulation", True, {
            "categories_tested": len(tool_categories),
            "simulation_results": simulation_results
        })
        
        return True
        
    except Exception as e:
        print(f"   âŒ Tool execution simulation failed: {str(e)}")
        log_test_result("tool_execution_simulation", False, {"error": str(e)})
        return False

async def test_error_handling():
    """Test error handling and fallback mechanisms"""
    print("\nâš ï¸ Testing Error Handling and Fallbacks...")
    
    try:
        # Test with invalid project ID
        invalid_project_id = "invalid_project_999"
        
        response = requests.get(
            f"{BASE_URL}/api/v1/mcp-ai/capabilities/{invalid_project_id}",
            headers=get_auth_headers(),
            timeout=30
        )
        
        # Log the response
        response_data = log_api_response(f"/api/v1/mcp-ai/capabilities/{invalid_project_id}", response, "error_handling")
        
        if response.status_code in [400, 404, 500]:
            print(f"   âœ… Error handling working: {response.status_code}")
            print(f"   âœ… System gracefully handled invalid project ID")
            
            log_test_result("error_handling", True, {
                "test_type": "invalid_project_id",
                "expected_error": True,
                "status_code": response.status_code
            })
            
        else:
            print(f"   âš ï¸ Unexpected response: {response.status_code}")
            
            log_test_result("error_handling", False, {
                "test_type": "invalid_project_id",
                "unexpected_status": response.status_code
            })
        
        # Test with malformed request
        malformed_request = {
            "invalid_field": "invalid_value"
        }
        
        response = requests.post(
            f"{BASE_URL}/api/v1/mcp-ai/process",
            headers={**get_auth_headers(), "Content-Type": "application/json"},
            json=malformed_request,
            timeout=30
        )
        
        # Log the response
        response_data = log_api_response("/api/v1/mcp-ai/process", response, "error_handling_malformed")
        
        if response.status_code in [400, 422]:
            print(f"   âœ… Malformed request handling working: {response.status_code}")
            
            log_test_result("error_handling", True, {
                "test_type": "malformed_request",
                "expected_error": True,
                "status_code": response.status_code
            })
            
        else:
            print(f"   âš ï¸ Unexpected response to malformed request: {response.status_code}")
            
            log_test_result("error_handling", False, {
                "test_type": "malformed_request",
                "unexpected_status": response.status_code
            })
        
        return True
        
    except Exception as e:
        print(f"   âŒ Error handling test failed: {str(e)}")
        log_test_result("error_handling", False, {"error": str(e)})
        return False

async def test_performance_metrics():
    """Test system performance metrics"""
    print("\nğŸ“ˆ Testing Performance Metrics...")
    
    try:
        performance_results = {
            "test_timestamp": datetime.now().isoformat(),
            "endpoint_response_times": {},
            "system_resources": {}
        }
        
        # Test response times for key endpoints
        endpoints_to_test = [
            "/api/v1/mcp-ai/test",
            "/api/v1/mcp-ai/tools",
            "/api/v1/mcp-ai/execution-history"
        ]
        
        for endpoint in endpoints_to_test:
            start_time = time.time()
            try:
                response = requests.get(f"{BASE_URL}{endpoint}", timeout=30)
                end_time = time.time()
                response_time = (end_time - start_time) * 1000  # Convert to milliseconds
                
                performance_results["endpoint_response_times"][endpoint] = {
                    "response_time_ms": round(response_time, 2),
                    "status_code": response.status_code,
                    "success": response.status_code == 200
                }
                
                print(f"   âœ… {endpoint}: {round(response_time, 2)}ms")
                
            except Exception as e:
                performance_results["endpoint_response_times"][endpoint] = {
                    "error": str(e),
                    "success": False
                }
                print(f"   âŒ {endpoint}: Failed - {str(e)}")
        
        # Log performance results
        log_to_file("system_logs/performance_metrics.json", json.dumps(performance_results, indent=2))
        
        # Calculate average response time
        successful_tests = [
            result for result in performance_results["endpoint_response_times"].values()
            if result.get("success", False)
        ]
        
        if successful_tests:
            avg_response_time = sum(result["response_time_ms"] for result in successful_tests) / len(successful_tests)
            print(f"   ğŸ“Š Average Response Time: {round(avg_response_time, 2)}ms")
            
            log_test_result("performance_metrics", True, {
                "average_response_time_ms": round(avg_response_time, 2),
                "total_endpoints_tested": len(endpoints_to_test),
                "successful_tests": len(successful_tests)
            })
        else:
            print("   âŒ No successful performance tests")
            log_test_result("performance_metrics", False, {"error": "No successful tests"})
        
        return len(successful_tests) > 0
        
    except Exception as e:
        print(f"   âŒ Performance metrics test failed: {str(e)}")
        log_test_result("performance_metrics", False, {"error": str(e)})
        return False

async def run_comprehensive_test():
    """Run the complete comprehensive test suite"""
    print("ğŸš€ Starting Comprehensive MCP AI Agent System Test")
    print("=" * 80)
    print(f"ğŸ“… Test Session Started: {test_session['start_time']}")
    print(f"ğŸ“ Test Results Directory: {TEST_RESULTS_DIR}")
    print("=" * 80)
    
    # Create test results directory if it doesn't exist
    os.makedirs(TEST_RESULTS_DIR, exist_ok=True)
    
    # Log test session start
    log_to_file("system_logs/test_session.json", json.dumps(test_session, indent=2))
    
    try:
        # Run all tests
        tests = [
            ("System Health", test_system_health),
            ("MCP AI Agent Status", test_mcp_ai_agent_status),
            ("Tool Discovery", test_tool_discovery),
            ("Execution History", test_execution_history),
            ("Natural Language Processing", test_natural_language_processing),
            ("Context Generation", test_context_generation),
            ("Tool Execution Simulation", test_tool_execution_simulation),
            ("Error Handling", test_error_handling),
            ("Performance Metrics", test_performance_metrics)
        ]
        
        for test_name, test_func in tests:
            print(f"\n{'='*60}")
            print(f"ğŸ§ª Running Test: {test_name}")
            print(f"{'='*60}")
            
            try:
                result = await test_func()
                if result:
                    print(f"âœ… {test_name} - PASSED")
                else:
                    print(f"âŒ {test_name} - FAILED")
            except Exception as e:
                print(f"ğŸ’¥ {test_name} - ERROR: {str(e)}")
                test_session["errors"].append({
                    "test_name": test_name,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
        
        # Generate comprehensive test report
        await generate_test_report()
        
        print(f"\n{'='*80}")
        print("ğŸ‰ Comprehensive Test Suite Completed!")
        print(f"{'='*80}")
        print(f"ğŸ“Š Test Results Summary:")
        print(f"   âœ… Successful Tests: {test_session['success_count']}")
        print(f"   âŒ Failed Tests: {test_session['failure_count']}")
        print(f"   âš ï¸ Errors: {len(test_session['errors'])}")
        print(f"   ğŸ“ Results saved to: {TEST_RESULTS_DIR}")
        
    except Exception as e:
        print(f"\nğŸ’¥ Test suite failed: {str(e)}")
        test_session["errors"].append({
            "test_name": "test_suite",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        })

async def generate_test_report():
    """Generate comprehensive test report"""
    print("\nğŸ“‹ Generating Test Report...")
    
    # Update test session with end time
    test_session["end_time"] = datetime.now().isoformat()
    test_session["total_duration"] = (
        datetime.fromisoformat(test_session["end_time"]) - 
        datetime.fromisoformat(test_session["start_time"])
    ).total_seconds()
    
    # Generate summary report
    summary_report = {
        "test_session": test_session,
        "system_info": {
            "backend_url": BASE_URL,
            "test_timestamp": datetime.now().isoformat(),
            "python_version": sys.version,
            "platform": sys.platform
        },
        "test_summary": {
            "total_tests": len(test_session["tests_run"]),
            "success_rate": f"{(test_session['success_count'] / len(test_session['tests_run'])) * 100:.1f}%" if test_session['tests_run'] else "0%",
            "critical_errors": len([e for e in test_session["errors"] if "critical" in e.get("test_name", "").lower()])
        }
    }
    
    # Log comprehensive report
    log_to_file("system_logs/comprehensive_test_report.json", json.dumps(summary_report, indent=2))
    
    # Generate human-readable report
    human_report = f"""
# ğŸš€ MCP AI Agent System - Comprehensive Test Report

## ğŸ“Š Test Summary
- **Test Session ID**: {test_session['start_time']}
- **Total Tests Run**: {len(test_session['tests_run'])}
- **Successful Tests**: {test_session['success_count']}
- **Failed Tests**: {test_session['failure_count']}
- **Success Rate**: {summary_report['test_summary']['success_rate']}
- **Total Duration**: {test_session['total_duration']:.2f} seconds

## ğŸ§ª Test Results

"""
    
    for test in test_session["tests_run"]:
        status = "âœ… PASS" if test["success"] else "âŒ FAIL"
        human_report += f"- **{test['name']}**: {status}\n"
    
    if test_session["errors"]:
        human_report += "\n## âš ï¸ Errors Encountered\n"
        for error in test_session["errors"]:
            human_report += f"- **{error['test_name']}**: {error['error']}\n"
    
    human_report += f"""
## ğŸ“ Test Artifacts
All test results have been saved to the following directories:
- **AI Logs**: `{TEST_RESULTS_DIR}/ai_logs/`
- **Cell Executions**: `{TEST_RESULTS_DIR}/cell_executions/`
- **Actions**: `{TEST_RESULTS_DIR}/actions/`
- **Outputs**: `{TEST_RESULTS_DIR}/outputs/`
- **Visualizations**: `{TEST_RESULTS_DIR}/visualizations/`
- **API Responses**: `{TEST_RESULTS_DIR}/api_responses/`
- **System Logs**: `{TEST_RESULTS_DIR}/system_logs/`

## ğŸ¯ Recommendations
"""
    
    if test_session['success_count'] > test_session['failure_count']:
        human_report += "- âœ… System is functioning well with most tests passing\n"
        human_report += "- ğŸ”§ Address any failed tests for optimal performance\n"
    else:
        human_report += "- âš ï¸ Multiple test failures detected - system needs attention\n"
        human_report += "- ğŸ” Review error logs for root cause analysis\n"
    
    human_report += f"""
## ğŸ“… Next Steps
1. Review detailed logs in each test results directory
2. Address any critical errors or failures
3. Validate system functionality in production environment
4. Schedule follow-up testing if needed

---
*Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    log_to_file("system_logs/test_report.md", human_report)
    
    print("   âœ… Test report generated successfully")

if __name__ == "__main__":
    # Run the comprehensive test
    asyncio.run(run_comprehensive_test())
